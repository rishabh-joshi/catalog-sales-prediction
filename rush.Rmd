---
title: "Predictive Analytics Project"
author: "Rush"
output: 
    html_document: 
        keep_md: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(comment = "")
knitr::opts_chunk$set(fig.align = 'center')
```

## Data Preprocessing

Loading required libraries
```{r, warning=FALSE, message=FALSE}
library(dplyr)
library(lubridate)
library(ggplot2)
library(tidyr)
library(reshape2)
library(car)
library(bestglm)
library(caret)
library(glmnet)
library(pROC)
library(SDMTools)
```

Reading the data
```{r}
train <- read.csv("clean_train.csv", stringsAsFactors = FALSE)
test <- read.csv("clean_test.csv", stringsAsFactors = FALSE)
```


Converting both the date variabels to their corresponding integer values, which specifies the number of days since the date 1970-01-01. I am doing this to make the date a continuous variable.

```{r}
train <- train %>%
    mutate(large_avg = factor(large_avg), pur3yr = factor(pur3yr)) %>%
    mutate(lpurseason = factor(lpurseason), slscmp = factor(slscmp)) %>%
    mutate(id = as.character(id))

test <- test %>%
    mutate(large_avg = factor(large_avg), pur3yr = factor(pur3yr)) %>%
    mutate(lpurseason = factor(lpurseason), slscmp = factor(slscmp)) %>%
    mutate(id = as.character(id))
```

Removing last purchase year from the data because we have already accounted for the last purchase date in the `datelp6` variable. We also need to remove the `responded` variable from this training data because responded will always be 1 in this case since we are only considering `targdol>0`.

```{r}
train_logistic <- select(train, -train, -targdol, -lpuryear, -lpurseason_year)
test_logistic <- select(test, -train, -targdol, -lpuryear, -lpurseason_year)

train_linear <- train %>%
    filter(responded==1) %>%
    select(-datead6, -datelp6) %>%
    select(-train, -lpuryear, -responded, -lpurseason_year, -recency_bin) 

test_linear <- test %>%
    select(-datead6, -datelp6) %>%
    select(-train, -lpuryear, -responded, -lpurseason_year, -recency_bin)
```

Creating a centered training and testing data set.
```{r}
train_linear_cent <- train_linear %>%
    mutate(slstyr = log(slstyr+K), slslyr = log(slslyr+K), 
           sls2ago = log(sls2ago+K), sls3ago = log(sls3ago+K), 
           sls4bfr = log(sls4bfr+K), slshist = log(slshist+K), 
           ordtyr = log(ordtyr+K), ordlyr = log(ordlyr+K), 
           ord2ago = log(ord2ago+K), ord3ago = log(ord3ago+K), 
           ord4bfr = log(ord4bfr+K), ordhist = log(ordhist+K), 
           falord = log(falord+K), sprord = log(sprord+K), 
           avg_amount = log(avg_amount+K))

ind <- sapply(train_linear_cent, is.numeric)
ind[1] <- FALSE
temp <- sapply(train_linear_cent[ind], scale, scale = FALSE)
temp <- data.frame(temp)
train_linear_cent <- cbind(temp, train_linear_cent[!ind])

test_linear_cent <- test_linear %>%
    mutate(slstyr = log(slstyr+K), slslyr = log(slslyr+K), 
           sls2ago = log(sls2ago+K), sls3ago = log(sls3ago+K), 
           sls4bfr = log(sls4bfr+K), slshist = log(slshist+K), 
           ordtyr = log(ordtyr+K), ordlyr = log(ordlyr+K), 
           ord2ago = log(ord2ago+K), ord3ago = log(ord3ago+K), 
           ord4bfr = log(ord4bfr+K), ordhist = log(ordhist+K), 
           falord = log(falord+K), sprord = log(sprord+K), 
           avg_amount = log(avg_amount+K))

ind <- sapply(test_linear_cent, is.numeric)
ind[1] <- FALSE
temp <- sapply(test_linear_cent[ind], scale, scale = FALSE)
temp <- data.frame(temp)
test_linear_cent <- cbind(temp, test_linear_cent[!ind])
```


Checking for correlation between the predictors
```{r}
tr <- select(train_linear, -targdol, -lpurseason, -large_avg, -pur3yr, -slscmp)
tr_cor <- as.matrix(cor(tr))
tr_cor_melt <- arrange(melt(tr_cor), -abs(value))
tr_cor_melt <- filter(tr_cor_melt, Var1 != Var2)
tr_cor_melt <- tr_cor_melt[seq(from=1, to=nrow(tr_cor_melt), by=2),]
thresh = 0.5 # displaying only correlations above threshold
filter(tr_cor_melt, value>thresh)
```


## Plots

```{r}
ggplot(data = train, aes(x = datead6, y = targdol)) +
    geom_point()

ggplot(data = train, aes(x = datelp6, y = targdol)) +
    geom_point()

ggplot(data = train, aes(x = slstyr, y = targdol)) +
    geom_point()

ggplot(data = train, aes(x = slslyr, y = targdol)) +
    geom_point()

ggplot(data = train, aes(x = sls2ago, y = targdol)) +
    geom_point()

ggplot(data = train, aes(x = sls3ago, y = targdol)) +
    geom_point()

ggplot(data = train, aes(x = log(slshist+1), y = targdol)) +
    geom_point()

ggplot(data = train, aes(x = log(ordtyr+1), y = targdol)) +
    geom_point()

ggplot(data = train, aes(x = ordlyr, y = targdol)) +
    geom_point()

ggplot(data = train, aes(x = ord2ago, y = targdol)) +
    geom_point()

ggplot(data = train, aes(x = ord3ago, y = targdol)) +
    geom_point()

ggplot(data = train, aes(x = ordhist, y = targdol)) +
    geom_point()

ggplot(data = train, aes(x = lpurseason, y = targdol)) +
    geom_point()

ggplot(data = train, aes(x = ordtyr, y = targdol)) +
    geom_point()
```

Checking univariate distributions of the variables
```{r}
qplot(x = targdol, data = train_linear, geom = "histogram")
qplot(x = datead6, data = train_linear, geom = "histogram")
qplot(x = datelp6, data = train_linear, geom = "histogram")
qplot(x = slstyr, data = train_linear, geom = "histogram")
qplot(x = slslyr, data = train_linear, geom = "histogram")
qplot(x = sls2ago, data = train_linear, geom = "histogram")
qplot(x = sls3ago, data = train_linear, geom = "histogram")
qplot(x = slshist, data = train_linear, geom = "histogram")
qplot(x = ordtyr, data = train_linear, geom = "histogram")
qplot(x = ordlyr, data = train_linear, geom = "histogram")
qplot(x = ord2ago, data = train_linear, geom = "histogram")
qplot(x = ord3ago, data = train_linear, geom = "histogram")
qplot(x = ordhist, data = train_linear, geom = "histogram")

qplot(x = falord, data = train_linear, geom = "histogram")
qplot(x = sprord, data = train_linear, geom = "histogram")
qplot(x = recency_bin, data = train_linear, geom = "histogram")
qplot(x = recency, data = train_linear, geom = "histogram")
qplot(x = lifetime, data = train_linear, geom = "histogram")
qplot(x = active, data = train_linear, geom = "histogram")
qplot(x = avg_amount, data = train_linear, geom = "histogram")
# qplot(x = large_avg, data = train, geom = "histogram")
# qplot(x = lpurseason, data = train, geom = "histogram")
qplot(x = ord4bfr, data = train_linear, geom = "histogram")
qplot(x = sls4bfr, data = train_linear, geom = "histogram")
```

Checking distributions after log transformation. Adding a small displacement to avoid taking log of zeros.

A small constant to add before taking log
```{r}
K = 0.0001
```


```{r}
qplot(x = log(targdol + K), data = train, geom = "histogram")
qplot(x = log(datead6 + K), data = train, geom = "histogram")
qplot(x = log(datelp6 + K), data = train, geom = "histogram")
qplot(x = log(slstyr + K), data = train, geom = "histogram")
qplot(x = log(slslyr + K), data = train, geom = "histogram")
qplot(x = log(sls2ago + K), data = train, geom = "histogram")
qplot(x = log(sls3ago + K), data = train, geom = "histogram")
qplot(x = log(slshist + K), data = train, geom = "histogram")
qplot(x = log(ordtyr + K), data = train, geom = "histogram")
qplot(x = log(ordlyr + K), data = train, geom = "histogram")
qplot(x = log(ord2ago + K), data = train, geom = "histogram")
qplot(x = log(ord3ago + K), data = train, geom = "histogram")
qplot(x = log(ordhist + K), data = train, geom = "histogram")

qplot(x = log(falord + K), data = train, geom = "histogram")
qplot(x = log(sprord + K), data = train, geom = "histogram")
qplot(x = log(recency_bin + K), data = train, geom = "histogram")
# qplot(x = log(recency + K), data = train, geom = "histogram")
# qplot(x = log(lifetime + K), data = train, geom = "histogram")
# qplot(x = log(active + K), data = train, geom = "histogram")
qplot(x = log(avg_amount + K), data = train, geom = "histogram")
# qplot(x = log(large_avg + K), data = train, geom = "histogram")
qplot(x = log(ord4bfr + K), data = train, geom = "histogram")
qplot(x = log(sls4bfr + K), data = train, geom = "histogram")
```

## Logistic regression models

### Fitting a basic logistic regression model

The variables not included in this model are becuase they were introducing multicollinearity and hence resulting in NA values while fitting the model.
```{r}
logistic1 <- glm(responded ~ . - ordhist - slshist - recency_bin - datelp6 - datead6 - ord4bfr, data = select(train_logistic, -id), 
              family = binomial)
summary(logistic1)
```

Checking the CCR on the test data
```{r}
pred <- fitted(logistic1)

pred_test <- predict(logistic1, newdata = select(test_logistic, -id), 
                     type = 'response')
acc <- 0
thresh <- 0
for (i in seq(0,1,0.01)){
    tab <- table(test_logistic$responded, pred_test>i)
    CCR <- sum(diag(tab))/sum(tab)
    if(CCR > acc){
        acc <- CCR
        thresh <- i
        cat(paste(thresh, acc, " "), "\n")
    }
}

thresh
caret::confusionMatrix(train_logistic$responded, as.integer(pred>thresh))
```

Checking the CCR on the test data
```{r}
caret::confusionMatrix(test_logistic$responded, as.integer(pred_test>thresh))
```

**CCR in the test data = 0.9152**

Checking the F1 score of this logistic model
```{r}
F_meas(factor(as.integer(pred_test>thresh)), 
       reference = as.factor(test_logistic$responded), relevant = "1")
```

**F1 score = 0.314**

### Fitting a logistic model with log of the predictors

```{r}
logistic2 <- glm(responded ~ I(log(slstyr + K)) + I(log(slslyr + K)) +
                I(log(sls2ago + K)) + I(log(sls3ago + K)) + I(log(sls4bfr + K)) +
                I(log(ordtyr + K)) + I(log(ordlyr + K)) +
                I(log(ord2ago + K)) + I(log(ord3ago + K)) + I(log(ord4bfr + K)) +
                I(log(sprord+K)) + I(log(falord+K)) +
                I(log(avg_amount + K)) + large_avg + active + lifetime + 
                recency + lpurseason, data = select(train_logistic, -id), 
        family = binomial)
summary(logistic2)
```

Checking the CCR on the test data
```{r}
pred <- fitted(logistic2)

pred_test <- predict(logistic2, newdata = select(test_logistic, -id), 
                     type = 'response')
acc <- 0
thresh <- 0
for (i in seq(0,1,0.01)){
    tab <- table(test_logistic$responded, pred_test>i)
    CCR <- sum(diag(tab))/sum(tab)
    if(CCR > acc){
        acc <- CCR
        thresh <- i
        cat(paste(thresh, acc, " "), "\n")
    }
}

thresh
caret::confusionMatrix(train_logistic$responded, as.integer(pred>thresh))
```

Checking the CCR on the test data
```{r}
caret::confusionMatrix(test_logistic$responded, as.integer(pred_test>thresh))
```

**CCR on the test data = 0.9242**

Checking the F1 score of this logistic model
```{r}
F_meas(factor(as.integer(pred_test>thresh)), 
       reference = as.factor(test_logistic$responded), relevant = "1")
```

**F1 score on the test data = 0.39**

Significant improvement from the previous model. We definitely should regress on the log of predictors.

### Adding interactions between sales

We add some interactions between sales of last 1, 2 and 3 years to capture consistency. I added these interactions one by one to see if they increase the CCR and the F1 Score on the test set. We kept all the interactions that resulted in an increase in the CCR and F1 Score. Adding similar kinds of interactions between the orders of last 1,2 or 3 years did not increase the F1 score or the CCR. Therefore, we did not add them.

```{r}
logistic3 <- glm(responded ~ I(log(slstyr + K)) + I(log(slslyr + K)) +
                I(log(sls2ago + K)) + I(log(sls3ago + K)) + I(log(sls4bfr + K)) +
                I(log(ordtyr + K)) + I(log(ordlyr + K)) +
                I(log(ord2ago + K)) + I(log(ord3ago + K)) + I(log(ord4bfr + K)) +
                I(log(sprord+K)) + I(log(falord+K)) +
                I(log(avg_amount + K)) + large_avg + active + lifetime + 
                recency + lpurseason + 
                I(log(slstyr + K)):I(log(slslyr + K)) +
                I(log(slslyr + K)):I(log(sls2ago + K)) + 
                I(log(sls2ago + K)):I(log(sls3ago + K)) +
                I(log(sls3ago + K)):I(log(sls4bfr + K)), 
                data = select(train_logistic, -id), 
        family = binomial)
summary(logistic3)
```

Checking the CCR on the test data
```{r}
pred <- fitted(logistic3)
pred_test <- predict(logistic3, newdata = select(test_logistic, -id), 
                     type = 'response')
acc <- 0
thresh <- 0
for (i in seq(0,1,0.01)){
    tab <- table(test_logistic$responded, pred_test>i)
    CCR <- sum(diag(tab))/sum(tab)
    if(CCR > acc){
        acc <- CCR
        thresh <- i
        cat(paste(thresh, acc, " "), "\n")
    }
}

thresh
caret::confusionMatrix(train_logistic$responded, as.integer(pred>thresh))
```

Checking the CCR on the test data
```{r}
caret::confusionMatrix(test_logistic$responded, as.integer(pred_test>thresh))
```

**CCR on the test data = 0.9276**

Checking the F1 score of this logistic model
```{r}
F_meas(factor(as.integer(pred_test>thresh)), 
       reference = as.factor(test_logistic$responded), relevant = "1")
```

**F1 score on the test data = 0.4479**

Significant improvement from the previous model. Therefore we should keep these interactions in the losgistic model.

### Adding some more interactions

```{r}
logistic4 <- glm(responded ~ I(log(slstyr + K)) + I(log(slslyr + K)) +
                I(log(sls2ago + K)) + I(log(sls3ago + K)) + I(log(sls4bfr + K)) +
                I(log(ordtyr + K)) + I(log(ordlyr + K)) +
                I(log(ord2ago + K)) + I(log(ord3ago + K)) + I(log(ord4bfr + K)) +
                I(log(sprord+K)) + I(log(falord+K)) +
                I(log(avg_amount + K)) + large_avg + active + lifetime + 
                recency + lpurseason + 
                I(log(slstyr + K)):I(log(slslyr + K)) +
                I(log(slslyr + K)):I(log(sls2ago + K)) + 
                I(log(sls2ago + K)):I(log(sls3ago + K)) +
                I(log(sls3ago + K)):I(log(sls4bfr + K)) +
            
                recency:active + lifetime:slscmp + 
                I(log(slstyr + K)):I(log(ordtyr+ K)) + 
                I(log(slstyr + K)):recency, 
                data = select(train_logistic, -id), 
        family = binomial)
summary(logistic4)
```

Checking the CCR on the train data
```{r}
pred <- fitted(logistic4)
pred_test <- predict(logistic4, newdata = select(test_logistic, -id), 
                     type = 'response')
acc <- 0
thresh <- 0
for (i in seq(0,1,0.01)){
    tab <- table(test_logistic$responded, pred_test>i)
    CCR <- sum(diag(tab))/sum(tab)
    if(CCR > acc){
        acc <- CCR
        thresh <- i
        cat(paste(thresh, acc, " "), "\n")
    }
}

thresh
caret::confusionMatrix(train_logistic$responded, as.integer(pred>thresh))
```

Checking the CCR on the test data
```{r}
caret::confusionMatrix(test_logistic$responded, as.integer(pred_test>thresh))
```

**CCR on the test data = 0.9283**

Checking the F1 score of this logistic model
```{r}
F_meas(factor(as.integer(pred_test>thresh)), 
       reference = as.factor(test_logistic$responded), relevant = "1")
```

**F1 score on the test data = 0.4613**

Significant improvement from the previous model. Therefore we should keep these interactions in the losgistic model.


### Adding some more interactions

Adding these interactions increased the CCR but resulted in a decrease in the F1 score.

```{r}
logistic5 <- glm(responded ~ I(log(slstyr + K)) + I(log(slslyr + K)) +
                I(log(sls2ago + K)) + I(log(sls3ago + K)) + I(log(sls4bfr + K)) +
                I(log(ordtyr + K)) + I(log(ordlyr + K)) +
                I(log(ord2ago + K)) + I(log(ord3ago + K)) + I(log(ord4bfr + K)) +
                I(log(sprord+K)) + I(log(falord+K)) +
                I(log(avg_amount + K)) + large_avg + active + lifetime + 
                recency + lpurseason + 
                I(log(slstyr + K)):I(log(slslyr + K)) +
                I(log(slslyr + K)):I(log(sls2ago + K)) + 
                I(log(sls2ago + K)):I(log(sls3ago + K)) +
                I(log(sls3ago + K)):I(log(sls4bfr + K)) +
            
                recency:active + lifetime:slscmp + 
                I(log(slstyr + K)):I(log(ordtyr+ K)) + 
                I(log(slstyr + K)):recency + 
                    
                    I(log(slslyr + K)):recency +
                    I(log(sls2ago + K)):I(log(ord2ago + K)) + 
                    I(log(avg_amount + K)):recency, 
                data = select(train_logistic, -id), 
        family = binomial)
summary(logistic5)
```

Checking the CCR on the train data
```{r}
pred <- fitted(logistic5)
pred_test <- predict(logistic5, newdata = select(test_logistic, -id), 
                     type = 'response')
acc <- 0
thresh <- 0
for (i in seq(0,1,0.01)){
    tab <- table(test_logistic$responded, pred_test>i)
    CCR <- sum(diag(tab))/sum(tab)
    if(CCR > acc){
        acc <- CCR
        thresh <- i
        cat(paste(thresh, acc, " "), "\n")
    }
}

thresh
caret::confusionMatrix(train_logistic$responded, as.integer(pred>thresh))
```

Checking the CCR on the test data
```{r}
caret::confusionMatrix(test_logistic$responded, as.integer(pred_test>thresh))
```

**CCR on the test data = 0.929**

Checking the F1 score of this logistic model
```{r}
F_meas(factor(as.integer(pred_test>thresh)), 
       reference = as.factor(test_logistic$responded), relevant = "1")
```

**F1 score on the test data = 0.4444**

### Summary of all the logistic regression models

The CCR/accuracies and F1 scores of all the logistic regression models are given below:

- logistic1: CCR = 0.9152, F1 Score = 0.314 AIC = 25934
- logistic2: CCR = 0.9242, F1 Score = 0.39 AIC = 24332
- logistic3: CCR = 0.9276, F1 Score = 0.4479, AIC = 24080 
- logistic4: CCR = 0.9283, F1 Score = 0.4613, AIC = 23992
- logistic5: CCR = 0.929, F1 Score = 0.4444, AIC = 23873


### Writing the predicted probabilites to a file

```{r}
# prob_test <- data.frame(id = test_logistic$id, prob = round(pred_test, 3), responded = test_logistic$responded)
# prob_test <- arrange(prob_test, desc(prob))

prob_test <- data.frame(id = test_logistic$id, prob = pred_test)
write.csv(prob_test, "prob.csv", row.names=FALSE)
```



## Multiple Regression Model

Fitting a multiple regression model on `targdol` as response but only where `targdol>0`. 

### linear1

Fitting the linear model with targdol as reponse
```{r}
linear1 <- lm(targdol ~ . , data = select(train_linear, -id, -slshist, - ordhist, - ord4bfr))

summary(linear1)

pred_test <- predict(linear1, newdata = select(test_linear, -id, -slshist, - ordhist, 
                                               - ord4bfr))

targdol_test <- data.frame(id = test_linear$id, targdol = pred_test)
```

### linear2

```{r}
linear2 <- lm(I(log(targdol+K)) ~ I(log(slstyr + K)) + I(log(slslyr + K)) +
                  I(log(sls2ago + K)) + I(log(sls3ago + K)) + 
                  I(log(slshist + K)) + I(log(ordtyr + K)) + 
                  I(log(ordlyr + K)) + I(log(ord2ago + K)) + 
                  I(log(ord3ago + K)) + I(log(ordhist + K)) +
                  I(log(sprord+K)) + I(log(falord+K)) + 
                  I(log(avg_amount + K)) + large_avg + active + 
                  lifetime + recency + lpurseason +
                  
                  recency:active + 
                  I(log(ord2ago + K)):I(log(ord3ago + K)) +
                I(log(slslyr + K)):I(log(sls2ago + K)) + 
                I(log(sls2ago + K)):I(log(sls3ago + K)) +
                I(log(ord3ago + K)):I(log(ord4bfr + K)) +
                  I(log(avg_amount + K)):recency + 
                  pur3yr 
              ,data = select(train_linear, -id))

summary(linear2)

pred_test <- predict(linear2, newdata = test_linear)

targdol_test <- data.frame(id = test_linear$id, targdol = pred_test)
```

### linear3

```{r}
linear3 <- lm(log(targdol+K) ~ log(slstyr + K) + log(slslyr + K) +
                  log(sls2ago + K) + log(sls3ago + K) + 
                  log(slshist + K) + log(ordtyr + K) + 
                  log(ordlyr + K) + log(ord2ago + K) + 
                  log(ord3ago + K) + log(ordhist + K) +
                  log(sprord+K) + log(falord+K) + 
                  log(avg_amount + K) + large_avg + active + 
                  lifetime + recency + lpurseason, 
              data = select(train_linear, -id))

summary(linear3)

pred_test <- predict(linear3, newdata = test_linear)
pred_test <- exp(pred_test)-K

targdol_test <- data.frame(id = test_linear$id, targdol = pred_test)
```

### linear4

```{r}
linear4 <- lm(I(log(targdol+K)) ~ I(log(slstyr + K)) + I(log(slslyr + K)) +
                  I(log(sls2ago + K)) + I(log(sls3ago + K)) + 
                  I(log(slshist + K)) + I(log(ordtyr + K)) + 
                  I(log(ordlyr + K)) + I(log(ord2ago + K)) + 
                  I(log(ord3ago + K)) + I(log(ordhist + K)) +
                  I(log(sprord+K)) + I(log(falord+K)) + 
                  I(log(avg_amount + K)) + large_avg + active + 
                  lifetime + recency + lpurseason + 
                  
                I(log(slstyr + K)):I(log(slslyr + K)) +
                I(log(slslyr + K)):I(log(sls2ago + K)) + 
                I(log(sls2ago + K)):I(log(sls3ago + K)) +
                I(log(sls3ago + K)):I(log(sls4bfr + K)) 
            
                # recency:active + lifetime:slscmp + 
                # I(log(slstyr + K)):I(log(ordtyr+ K)) + 
                # I(log(slstyr + K)):recency + 
                #     
                #     I(log(slslyr + K)):recency +
                #     I(log(sls2ago + K)):I(log(ord2ago + K)) + 
                #     I(log(avg_amount + K)):recency
              ,data = select(train_linear, -id))

summary(linear4)

pred_test <- predict(linear4, newdata = test_linear)
pred_test <- exp(pred_test)-K

targdol_test <- data.frame(id = test_linear$id, targdol = pred_test)
```


### linear5

```{r}
linear5 <- lm(targdol ~ I(log(slstyr + K)) + I(log(slslyr + K)) +
                  I(log(sls2ago + K)) + I(log(sls3ago + K)) + 
                  I(log(slshist + K)) + I(log(ordtyr + K)) + 
                  I(log(ordlyr + K)) + I(log(ord2ago + K)) + 
                  I(log(ord3ago + K)) + I(log(ordhist + K)) +
                  I(log(sprord+K)) + I(log(falord+K)) + 
                  I(log(avg_amount + K)) + large_avg + active + 
                  lifetime + recency + lpurseason + 

                I(log(slstyr + K)):I(log(slslyr + K)) +
                I(log(slslyr + K)):I(log(sls2ago + K)) +
                I(log(sls2ago + K)):I(log(sls3ago + K)) +
                I(log(sls3ago + K)):I(log(sls4bfr + K)) +
            
                recency:active + lifetime:slscmp +
                I(log(slstyr + K)):I(log(ordtyr+ K)) +
                I(log(slstyr + K)):recency +

                    I(log(slslyr + K)):recency +
                    I(log(sls2ago + K)):I(log(ord2ago + K)) +
                    I(log(avg_amount + K)):recency
              ,data = train_linear)

summary(linear5)

pred_test <- predict(linear5, newdata = test_linear)
# pred_test <- exp(pred_test)-K

targdol_test <- data.frame(id = test_linear$id, targdol = pred_test)
```


### linear6 (from all possible pairs of interactions)

```{r}
linear6 <- lm(log(train_linear$targdol + K) ~ log(slstyr + K) + 
                                            log(sls2ago + K) + 
                                            log(sls3ago + K) +
                                            log(ordlyr + K) +
                                            log(ord2ago + K) + 
                                            log(ord3ago + K) +
                                            log(falord + K) +
                                            active +
                                            lifetime +
                                            lpurseason + 
                                            log(avg_amount + K) + 
                                            pur3yr +
                                            # log(slstyr + K):log(sls4bfr + K) + 
                                            log(slstyr + K):log(ordtyr + K) + 

                                            log(slstyr + K):log(avg_amount + K) + 
                                            log(slstyr + K):slscmp + 
                                            log(slslyr + K):log(ordlyr + K) + 
                                            log(slslyr + K):log(avg_amount + K) + 
                                            log(slslyr + K):lifetime +
                                            log(sls2ago + K):log(ord2ago + K) + 
                                            # log(sls2ago + K):large_avg + 
                                            log(sls2ago + K):recency +
                                            # log(sls3ago + K):active +
                                            log(ordtyr + K):log(ordlyr + K) + 
                                            log(ordtyr + K):log(avg_amount + K) + 
                                            # log(ordlyr + K):active + 
                                            log(ordlyr + K):lifetime +
                                            # log(ord2ago + K):large_avg + 
                                            log(ord2ago + K):recency +
                                            # log(ord3ago + K):active + 
                                            log(avg_amount + K):large_avg + 
                                            log(avg_amount + K):active + 
                                            # log(avg_amount + K):avg_amount +
                                            active:recency + 
                                            active:pur3yr + 
                                            lifetime:recency + 
                                            lifetime:pur3yr + 
                                            log(avg_amount + K):pur3yr 
                                            ,data = train_linear)

summary(linear6)

pred_test <- predict(linear6, newdata = test_linear)
pred_test <- exp(pred_test)-K

targdol_test <- data.frame(id = test_linear$id, targdol = pred_test)
```


### linear7 (linear6 refined)

keeping only the coefficients resulting from doing stepwise rergession on linear6

```{r}
linear7 <- lm(log(train_linear$targdol + K) ~ log(slstyr + K) + 
                                            log(sls2ago + K) + 
                                            # log(sls3ago + K) +
                                            log(ordlyr + K) +
                                            log(ord2ago + K) + 
                                            # log(ord3ago + K) +
                                            log(falord + K) +
                                            active +
                                            lifetime +
                                            lpurseason + 
                                            log(avg_amount + K) + 
                                            pur3yr +
                                            # log(slstyr + K):log(sls4bfr + K) + 
                                            log(slstyr + K):log(ordtyr + K) + 
                                            log(slstyr + K):log(avg_amount + K) + 
                                            log(slstyr + K):slscmp + 
                                            log(slslyr + K):log(ordlyr + K) + 
                                            log(slslyr + K):log(avg_amount + K) + 
                                            # log(slslyr + K):lifetime +
                                            log(sls2ago + K):log(ord2ago + K) +
                  # log(slslyr + K):log(sls2ago + K) +
                                            # log(sls2ago + K):large_avg + 
                                            # log(sls2ago + K):recency +
                                            # log(sls3ago + K):active +
                                            log(ordtyr + K):log(ordlyr + K) + 
                                            # log(ordtyr + K):log(avg_amount + K) + 
                                            # log(ordlyr + K):active + 
                                            # log(ordlyr + K):lifetime +
                                            # log(ord2ago + K):large_avg + 
                                            # log(ord2ago + K):recency +
                                            # log(ord3ago + K):active + 
                                            log(avg_amount + K):large_avg + 
                                            log(avg_amount + K):active + 
                                            # log(avg_amount + K):avg_amount +
                                            active:recency + 
                                            # active:pur3yr + 
                                            lifetime:recency + 
                                            # lifetime:pur3yr + 
                                            log(avg_amount + K):pur3yr 
                                            ,data = train_linear)

summary(linear7)

pred_test <- predict(linear7, newdata = test_linear)
pred_test <- exp(pred_test)-K

targdol_test <- data.frame(id = test_linear$id, targdol = pred_test)
```

### linear8 (linear7 but with centered predictors to remove multicollinearity)

Applied stepwise on linear7, kept only those predictors that were in the step model, then centered the predictors to remove multicollinearity.
```{r}
linear8 <- lm(log(train_linear$targdol + K) ~ I(log(slstyr + K) - mean(log(slstyr + K))) + 
            I(log(sls2ago + K) - mean(log(sls2ago + K))) +
            # log(sls3ago + K) +
            I(log(ordlyr + K) - mean(log(ordlyr + K))) +
            I(log(ord2ago + K) - mean(log(ord2ago + K))) +
            # log(ord3ago + K) +
            I(log(falord + K) - mean(log(falord + K))) +
            active +
            # lifetime +
            lpurseason + 
            I(log(avg_amount + K) - mean(log(avg_amount + K))) + 
            pur3yr +
            # log(slstyr + K):log(sls4bfr + K) + 
            I(log(slstyr + K) - mean(log(slstyr + K))):I(log(ordtyr + K) - mean(log(ordtyr + K))) + 
            I(log(slstyr + K) - mean(log(slstyr + K))):I(log(avg_amount + K) - mean(log(avg_amount + K))) + 
            I(log(slstyr + K) - mean(log(slstyr + K))):slscmp + 
            I(log(slslyr + K) - mean(log(slslyr + K))):I(log(ordlyr + K) - mean(log(ordlyr + K))) + 
            I(log(slslyr + K) - mean(log(slslyr + K))):I(log(avg_amount + K) - mean(log(avg_amount + K))) + 
            # log(slslyr + K):lifetime +
            I(log(sls2ago + K) - mean(log(sls2ago + K))):I(log(ord2ago + K) - mean(log(ord2ago + K))) +
# I(log(slslyr + K) - mean(log(slslyr + K))):I(log(sls2ago + K) - mean(log(sls2ago + K))) +
            # I(log(sls2ago + K) - mean(log(sls2ago + K))):large_avg + 
            # I(log(sls2ago + K) - mean(log(sls2ago + K))):recency +
            # I(log(sls3ago + K) - mean(log(sls3ago + K))):active +
            I(log(ordtyr + K) - mean(log(ordtyr + K))):I(log(ordlyr + K) - mean(log(ordlyr + K))) + 
            # log(ordtyr + K):log(avg_amount + K) + 
            # log(ordlyr + K):active + 
            # log(ordlyr + K):lifetime +
            # log(ord2ago + K):large_avg + 
            # log(ord2ago + K):recency +
            # log(ord3ago + K):active + 
            I(log(avg_amount + K) - mean(log(avg_amount + K))):large_avg + 
            I(log(avg_amount + K) - mean(log(avg_amount + K))):active + 
            # log(avg_amount + K):avg_amount +
            active:recency + 
            # active:pur3yr + 
            # lifetime:recency + 
            # lifetime:pur3yr + 
            I(log(avg_amount + K) - mean(log(avg_amount + K))):pur3yr 
                                            ,data = train_linear)

summary(linear8)

pred_test <- predict(linear8, newdata = test_linear)
pred_test <- exp(pred_test)-K

targdol_test <- data.frame(id = test_linear$id, targdol = pred_test)

```


### Stepwise regression

```{r}
linear_model <- linear8
bwd <- step(linear_model, direction = "both")
summary(bwd)

pred_test <- predict(bwd, newdata = test_linear)
# uncomment this line if the response of the model is log (for linear3 and linear4)
pred_test <- exp(pred_test)-K
targdol_test <- data.frame(id = test_linear_cent$id, targdol = pred_test)
```




### Writing the predicted targdol to a file

```{r}
write.csv(targdol_test, "targdol.csv", row.names=FALSE)
```

## Evalutaing the fitted models

Here we will evaluate the different combinations of the logistic and linear regression models that we have tried so far and see which of these models gets the lowest mean squared prediction error (statistical criterion) and the highest payoff (financial criterion).

```{r}
targdol_test <- read.csv("targdol.csv")
prob_test <- read.csv("prob.csv")
```

### Statistical Criterion (MSPE)

Calculating the MSPE using both logistic and linear models and the final prediction of tagdol.
```{r}
actual_targdol <- select(test_linear, id, targdol)
linear_model <- linear7 # change this to any linear model you tried
prob_targdol <- merge(prob_test, targdol_test, by = "id")
prob_targdol <- merge(prob_targdol, actual_targdol, by = "id")
names(prob_targdol) <- c("id", "prob", "pred_targdol", "actual_targdol")
prob_targdol <- mutate(prob_targdol, expected_targdol = prob*pred_targdol)
head(prob_targdol)
p <- length(linear_model$coefficients) - 1
n <- nrow(prob_targdol)
# MSPE <- sum((prob_targdol$actual_targdol -
#                  prob_targdol$expected_targdol)^2)/(n-(p+1))
# MSPE
```

Adj r sq
```{r}
summary(linear_model)$adj.r.squared
```


For non-centered data
```{r}
linear_model <- linear7
pred_test <- predict(linear_model, newdata = filter(test_linear, targdol>0))
pred_test <- exp(pred_test) - K
original <- filter(test_linear, targdol>0)$targdol
n <- length(pred_test)
p <- length(coef(linear_model))
MSPE <- sum((pred_test - original)^2)/(n-p-1)
MSPE
```


FOR Centered Data

Calculating the MSPE on the subset of test data where the targdol>0 and only using the prediction from the linear model.
```{r}
linear_model <- bwd
pred_test <- predict(linear_model, newdata = filter(test_linear_cent, targdol>0))
pred_test <- exp(pred_test) - K
original <- filter(test_linear_cent, targdol>0)$targdol
n <- length(pred_test)
p <- length(coef(linear_model))
MSPE <- sum((pred_test - original)^2)/(n-p-1)
MSPE
```


### Financial Criterion (Payoff)

Sort in descending order of the expected targdol and grab the corresponding top 1000 actual targdol values.
```{r}
prob_targdol_arranged <- arrange(prob_targdol, desc(expected_targdol))
payoff <- sum(prob_targdol_arranged[1:1000, ]$actual_targdol)
payoff
```


## Theoretical maximum payoff

```{r}
th_max_payoff <- sum(sort(test$targdol, decreasing = TRUE)[1:1000])
precentage <- 100*payoff/th_max_payoff
```

adj r sq = 0.1543
MSPE = 2755.433
Payoff = 52,429.4

## Findings

#### linear1

- a lot of insignificant predictors.
- adj r sq = 0.09153
- payoff = 52715.35
- mspe = 2495.93

Performing stepwise regression results in 

- a model with all the predictors significant
- adj r sq = 0.09183
- payoff = 53111.85
- mspe = 2495.983


#### linear2

- a lot of insignificant predictors.
- adj r sq = 0.06978
- payoff = 54585.74
- mspe = 2544.327

Performing stepwise regression results in 

- a model with all the predictors significant
- adj r sq = 0.07074
- payoff = 53806.49
- mspe = 2536.906

#### linear3 

- most predictors are already significant
- adj r sq = 0.1094
- payoff = 54008.29
- mspe = 2729.643


Performing stepwise regression results in

- adj r sq = 0.1095
- payoff = 53972.44
- mspe = 2729.299


#### linear4

- all the interaction terms added between sales are insignificant
- adj r sq = 0.1126
- payoff = 52958.24
- mspe = 2735.724


Performing stepwise regression results in

- all terms are significant but the interactions terms are removed
- adj r sq = 0.1132
- payoff = 53111.04
- mspe = 2734.543

#### linear5

- all the interaction terms added between sales are insignificant
- adj r sq = 0.07429
- payoff = 53546.74
- mspe = 2560.358


Performing stepwise regression results in

- all terms are significant
- adj r sq = 0.07539
- payoff = 53546.74
- mspe = 2553.414


#### linear2 with log response

- adj r sq = 0.1097
- payoff = 53734.59
- mspe = 2735.528