---
title: "Predictive Analytics Project"
author: "Rush"
output: 
    html_document: 
        keep_md: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(comment = "")
knitr::opts_chunk$set(fig.align = 'center')
```

## Data Preprocessing

Loading required libraries
```{r, warning=FALSE, message=FALSE}
library(dplyr)
library(lubridate)
library(ggplot2)
library(tidyr)
library(reshape2)
library(car)
library(bestglm)
library(caret)
library(glmnet)
library(pROC)
library(SDMTools)
```

Reading the data
```{r}
train <- read.csv("clean_train.csv", stringsAsFactors = FALSE)
test <- read.csv("clean_test.csv", stringsAsFactors = FALSE)
```


Converting both the date variabels to their corresponding integer values, which specifies the number of days since the date 1970-01-01. I am doing this to make the date a continuous variable.

```{r}
train <- train %>%
    mutate(datead6 = as.integer(ymd(datead6))) %>%
    mutate(datelp6 = as.integer(ymd(datelp6))) %>%
    mutate(large_avg = factor(large_avg), pur3yr = factor(pur3yr)) %>%
    mutate(lpurseason = factor(lpurseason), slscmp = factor(slscmp))

test <- test %>%
    mutate(datead6 = as.integer(ymd(datead6))) %>%
    mutate(datelp6 = as.integer(ymd(datelp6))) %>%
    mutate(large_avg = factor(large_avg), pur3yr = factor(pur3yr)) %>%
    mutate(lpurseason = factor(lpurseason), slscmp = factor(slscmp))
```

Removing last purchase year from the data because we have already accounted for the last purchase date in the `datelp6` variable. We also need to remove the `responded` variable from this training data because responded will always be 1 in this case since we are only considering `targdol>0`.

```{r}
train_logistic <- select(train, -train, -targdol, -lpuryear, -lpurseason_year)
test_logistic <- select(test, -train, -targdol, -lpuryear, -lpurseason_year)

train_linear <- train %>%
    filter(responded==1) %>%
    select(-train, -lpuryear, -responded, -lpurseason_year, -recency_bin) 

test_linear <- test %>%
    select(-train, -lpuryear, -responded, -lpurseason_year, -recency_bin)
```


Checking for correlation between the predictors
```{r}
tr <- select(train_linear, -targdol, -lpurseason, -large_avg, -pur3yr, -slscmp)
tr_cor <- as.matrix(cor(tr))
tr_cor_melt <- arrange(melt(tr_cor), -abs(value))
tr_cor_melt <- filter(tr_cor_melt, Var1 != Var2)
tr_cor_melt <- tr_cor_melt[seq(from=1, to=nrow(tr_cor_melt), by=2),]
thresh = 0.5 # displaying only correlations above threshold
filter(tr_cor_melt, value>thresh)
```


## Plots

```{r}
ggplot(data = train, aes(x = datead6, y = targdol)) +
    geom_point()

ggplot(data = train, aes(x = datelp6, y = targdol)) +
    geom_point()

ggplot(data = train, aes(x = slstyr, y = targdol)) +
    geom_point()

ggplot(data = train, aes(x = slslyr, y = targdol)) +
    geom_point()

ggplot(data = train, aes(x = sls2ago, y = targdol)) +
    geom_point()

ggplot(data = train, aes(x = sls3ago, y = targdol)) +
    geom_point()

ggplot(data = train, aes(x = log(slshist+1), y = targdol)) +
    geom_point()

ggplot(data = train, aes(x = log(ordtyr+1), y = targdol)) +
    geom_point()

ggplot(data = train, aes(x = ordlyr, y = targdol)) +
    geom_point()

ggplot(data = train, aes(x = ord2ago, y = targdol)) +
    geom_point()

ggplot(data = train, aes(x = ord3ago, y = targdol)) +
    geom_point()

ggplot(data = train, aes(x = ordhist, y = targdol)) +
    geom_point()

ggplot(data = train, aes(x = lpurseason, y = targdol)) +
    geom_point()

ggplot(data = train, aes(x = ordtyr, y = targdol)) +
    geom_point()
```

Checking univariate distributions of the variables
```{r}
qplot(x = targdol, data = train_linear, geom = "histogram")
qplot(x = datead6, data = train_linear, geom = "histogram")
qplot(x = datelp6, data = train_linear, geom = "histogram")
qplot(x = slstyr, data = train_linear, geom = "histogram")
qplot(x = slslyr, data = train_linear, geom = "histogram")
qplot(x = sls2ago, data = train_linear, geom = "histogram")
qplot(x = sls3ago, data = train_linear, geom = "histogram")
qplot(x = slshist, data = train_linear, geom = "histogram")
qplot(x = ordtyr, data = train_linear, geom = "histogram")
qplot(x = ordlyr, data = train_linear, geom = "histogram")
qplot(x = ord2ago, data = train_linear, geom = "histogram")
qplot(x = ord3ago, data = train_linear, geom = "histogram")
qplot(x = ordhist, data = train_linear, geom = "histogram")

qplot(x = falord, data = train_linear, geom = "histogram")
qplot(x = sprord, data = train_linear, geom = "histogram")
qplot(x = recency_bin, data = train_linear, geom = "histogram")
qplot(x = recency, data = train_linear, geom = "histogram")
qplot(x = lifetime, data = train_linear, geom = "histogram")
qplot(x = active, data = train_linear, geom = "histogram")
qplot(x = avg_amount, data = train_linear, geom = "histogram")
# qplot(x = large_avg, data = train, geom = "histogram")
# qplot(x = lpurseason, data = train, geom = "histogram")
qplot(x = ord4bfr, data = train_linear, geom = "histogram")
qplot(x = sls4bfr, data = train_linear, geom = "histogram")
```

Checking distributions after log transformation. Adding a small displacement to avoid taking log of zeros.

A small constant to add before taking log
```{r}
K = 0.0001
```


```{r}
qplot(x = log(targdol + K), data = train, geom = "histogram")
qplot(x = log(datead6 + K), data = train, geom = "histogram")
qplot(x = log(datelp6 + K), data = train, geom = "histogram")
qplot(x = log(slstyr + K), data = train, geom = "histogram")
qplot(x = log(slslyr + K), data = train, geom = "histogram")
qplot(x = log(sls2ago + K), data = train, geom = "histogram")
qplot(x = log(sls3ago + K), data = train, geom = "histogram")
qplot(x = log(slshist + K), data = train, geom = "histogram")
qplot(x = log(ordtyr + K), data = train, geom = "histogram")
qplot(x = log(ordlyr + K), data = train, geom = "histogram")
qplot(x = log(ord2ago + K), data = train, geom = "histogram")
qplot(x = log(ord3ago + K), data = train, geom = "histogram")
qplot(x = log(ordhist + K), data = train, geom = "histogram")

qplot(x = log(falord + K), data = train, geom = "histogram")
qplot(x = log(sprord + K), data = train, geom = "histogram")
qplot(x = log(recency_bin + K), data = train, geom = "histogram")
# qplot(x = log(recency + K), data = train, geom = "histogram")
# qplot(x = log(lifetime + K), data = train, geom = "histogram")
# qplot(x = log(active + K), data = train, geom = "histogram")
qplot(x = log(avg_amount + K), data = train, geom = "histogram")
# qplot(x = log(large_avg + K), data = train, geom = "histogram")
qplot(x = log(ord4bfr + K), data = train, geom = "histogram")
qplot(x = log(sls4bfr + K), data = train, geom = "histogram")
```

## Logistic regression models

### Fitting a basic logistic regression model

The variables not included in this model are becuase they were introducing multicollinearity and hence resulting in NA values while fitting the model.
```{r}
logistic1 <- glm(responded ~ . - ordhist - slshist - recency_bin - datelp6 - datead6 - ord4bfr, data = select(train_logistic, -id), 
              family = binomial)
summary(logistic1)
```

Checking the CCR on the test data
```{r}
pred <- fitted(logistic1)

pred_test <- predict(logistic1, newdata = select(test_logistic, -id), 
                     type = 'response')
acc <- 0
thresh <- 0
for (i in seq(0,1,0.01)){
    tab <- table(test_logistic$responded, pred_test>i)
    CCR <- sum(diag(tab))/sum(tab)
    if(CCR > acc){
        acc <- CCR
        thresh <- i
        cat(paste(thresh, acc, " "), "\n")
    }
}

thresh
caret::confusionMatrix(train_logistic$responded, as.integer(pred>thresh))
```

Checking the CCR on the test data
```{r}
caret::confusionMatrix(test_logistic$responded, as.integer(pred_test>thresh))
```

**CCR in the test data = 0.9152**

Checking the F1 score of this logistic model
```{r}
F_meas(factor(as.integer(pred_test>thresh)), 
       reference = as.factor(test_logistic$responded), relevant = "1")
```

**F1 score = 0.314**

### Fitting a logistic model with log of the predictors

```{r}
logistic2 <- glm(responded ~ I(log(slstyr + K)) + I(log(slslyr + K)) +
                I(log(sls2ago + K)) + I(log(sls3ago + K)) + I(log(sls4bfr + K)) +
                I(log(ordtyr + K)) + I(log(ordlyr + K)) +
                I(log(ord2ago + K)) + I(log(ord3ago + K)) + I(log(ord4bfr + K)) +
                I(log(sprord+K)) + I(log(falord+K)) +
                I(log(avg_amount + K)) + large_avg + active + lifetime + 
                recency + lpurseason, data = select(train_logistic, -id), 
        family = binomial)
summary(logistic2)
```

Checking the CCR on the test data
```{r}
pred <- fitted(logistic2)

pred_test <- predict(logistic2, newdata = select(test_logistic, -id), 
                     type = 'response')
acc <- 0
thresh <- 0
for (i in seq(0,1,0.01)){
    tab <- table(test_logistic$responded, pred_test>i)
    CCR <- sum(diag(tab))/sum(tab)
    if(CCR > acc){
        acc <- CCR
        thresh <- i
        cat(paste(thresh, acc, " "), "\n")
    }
}

thresh
caret::confusionMatrix(train_logistic$responded, as.integer(pred>thresh))
```

Checking the CCR on the test data
```{r}
caret::confusionMatrix(test_logistic$responded, as.integer(pred_test>thresh))
```

**CCR on the test data = 0.9242**

Checking the F1 score of this logistic model
```{r}
F_meas(factor(as.integer(pred_test>thresh)), 
       reference = as.factor(test_logistic$responded), relevant = "1")
```

**F1 score on the test data = 0.39**

Significant improvement from the previous model. We definitely should regress on the log of predictors.

### Adding interactions between sales

We add some interactions between sales of last 1, 2 and 3 years to capture consistency. I added these interactions one by one to see if they increase the CCR and the F1 Score on the test set. We kept all the interactions that resulted in an increase in the CCR and F1 Score. Adding similar kinds of interactions between the orders of last 1,2 or 3 years did not increase the F1 score or the CCR. Therefore, we did not add them.

```{r}
logistic3 <- glm(responded ~ I(log(slstyr + K)) + I(log(slslyr + K)) +
                I(log(sls2ago + K)) + I(log(sls3ago + K)) + I(log(sls4bfr + K)) +
                I(log(ordtyr + K)) + I(log(ordlyr + K)) +
                I(log(ord2ago + K)) + I(log(ord3ago + K)) + I(log(ord4bfr + K)) +
                I(log(sprord+K)) + I(log(falord+K)) +
                I(log(avg_amount + K)) + large_avg + active + lifetime + 
                recency + lpurseason + 
                I(log(slstyr + K)):I(log(slslyr + K)) +
                I(log(slslyr + K)):I(log(sls2ago + K)) + 
                I(log(sls2ago + K)):I(log(sls3ago + K)) +
                I(log(sls3ago + K)):I(log(sls4bfr + K)), 
                data = select(train_logistic, -id), 
        family = binomial)
summary(logistic3)
```

Checking the CCR on the test data
```{r}
pred <- fitted(logistic3)
pred_test <- predict(logistic3, newdata = select(test_logistic, -id), 
                     type = 'response')
acc <- 0
thresh <- 0
for (i in seq(0,1,0.01)){
    tab <- table(test_logistic$responded, pred_test>i)
    CCR <- sum(diag(tab))/sum(tab)
    if(CCR > acc){
        acc <- CCR
        thresh <- i
        cat(paste(thresh, acc, " "), "\n")
    }
}

thresh
caret::confusionMatrix(train_logistic$responded, as.integer(pred>thresh))
```

Checking the CCR on the test data
```{r}
caret::confusionMatrix(test_logistic$responded, as.integer(pred_test>thresh))
```

**CCR on the test data = 0.9276**

Checking the F1 score of this logistic model
```{r}
F_meas(factor(as.integer(pred_test>thresh)), 
       reference = as.factor(test_logistic$responded), relevant = "1")
```

**F1 score on the test data = 0.4479**

Significant improvement from the previous model. Therefore we should keep these interactions in the losgistic model.

### Adding some more interactions

```{r}
logistic4 <- glm(responded ~ I(log(slstyr + K)) + I(log(slslyr + K)) +
                I(log(sls2ago + K)) + I(log(sls3ago + K)) + I(log(sls4bfr + K)) +
                I(log(ordtyr + K)) + I(log(ordlyr + K)) +
                I(log(ord2ago + K)) + I(log(ord3ago + K)) + I(log(ord4bfr + K)) +
                I(log(sprord+K)) + I(log(falord+K)) +
                I(log(avg_amount + K)) + large_avg + active + lifetime + 
                recency + lpurseason + 
                I(log(slstyr + K)):I(log(slslyr + K)) +
                I(log(slslyr + K)):I(log(sls2ago + K)) + 
                I(log(sls2ago + K)):I(log(sls3ago + K)) +
                I(log(sls3ago + K)):I(log(sls4bfr + K)) +
            
                recency:active + lifetime:slscmp + 
                I(log(slstyr + K)):I(log(ordtyr+ K)) + 
                I(log(slstyr + K)):recency, 
                data = select(train_logistic, -id), 
        family = binomial)
summary(logistic4)
```

Checking the CCR on the train data
```{r}
pred <- fitted(logistic4)
pred_test <- predict(logistic4, newdata = select(test_logistic, -id), 
                     type = 'response')
acc <- 0
thresh <- 0
for (i in seq(0,1,0.01)){
    tab <- table(test_logistic$responded, pred_test>i)
    CCR <- sum(diag(tab))/sum(tab)
    if(CCR > acc){
        acc <- CCR
        thresh <- i
        cat(paste(thresh, acc, " "), "\n")
    }
}

thresh
caret::confusionMatrix(train_logistic$responded, as.integer(pred>thresh))
```

Checking the CCR on the test data
```{r}
caret::confusionMatrix(test_logistic$responded, as.integer(pred_test>thresh))
```

**CCR on the test data = 0.9283**

Checking the F1 score of this logistic model
```{r}
F_meas(factor(as.integer(pred_test>thresh)), 
       reference = as.factor(test_logistic$responded), relevant = "1")
```

**F1 score on the test data = 0.4613**

Significant improvement from the previous model. Therefore we should keep these interactions in the losgistic model.


### Adding some more interactions

Adding these interactions increased the CCR but resulted in a decrease in the F1 score.

```{r}
logistic5 <- glm(responded ~ I(log(slstyr + K)) + I(log(slslyr + K)) +
                I(log(sls2ago + K)) + I(log(sls3ago + K)) + I(log(sls4bfr + K)) +
                I(log(ordtyr + K)) + I(log(ordlyr + K)) +
                I(log(ord2ago + K)) + I(log(ord3ago + K)) + I(log(ord4bfr + K)) +
                I(log(sprord+K)) + I(log(falord+K)) +
                I(log(avg_amount + K)) + large_avg + active + lifetime + 
                recency + lpurseason + 
                I(log(slstyr + K)):I(log(slslyr + K)) +
                I(log(slslyr + K)):I(log(sls2ago + K)) + 
                I(log(sls2ago + K)):I(log(sls3ago + K)) +
                I(log(sls3ago + K)):I(log(sls4bfr + K)) +
            
                recency:active + lifetime:slscmp + 
                I(log(slstyr + K)):I(log(ordtyr+ K)) + 
                I(log(slstyr + K)):recency + 
                    
                    I(log(slslyr + K)):recency +
                    I(log(sls2ago + K)):I(log(ord2ago + K)) + 
                    I(log(avg_amount + K)):recency, 
                data = select(train_logistic, -id), 
        family = binomial)
summary(logistic5)
```

Checking the CCR on the train data
```{r}
pred <- fitted(logistic5)
pred_test <- predict(logistic5, newdata = select(test_logistic, -id), 
                     type = 'response')
acc <- 0
thresh <- 0
for (i in seq(0,1,0.01)){
    tab <- table(test_logistic$responded, pred_test>i)
    CCR <- sum(diag(tab))/sum(tab)
    if(CCR > acc){
        acc <- CCR
        thresh <- i
        cat(paste(thresh, acc, " "), "\n")
    }
}

thresh
caret::confusionMatrix(train_logistic$responded, as.integer(pred>thresh))
```

Checking the CCR on the test data
```{r}
caret::confusionMatrix(test_logistic$responded, as.integer(pred_test>thresh))
```

**CCR on the test data = 0.929**

Checking the F1 score of this logistic model
```{r}
F_meas(factor(as.integer(pred_test>thresh)), 
       reference = as.factor(test_logistic$responded), relevant = "1")
```

**F1 score on the test data = 0.4444**

### Summary of all the logistic regression models

The CCR/accuracies and F1 scores of all the logistic regression models are given below:

- logistic1: CCR = 0.9152, F1 Score = 0.314
- logistic2: CCR = 0.9242, F1 Score = 0.39
- logistic3: CCR = 0.9276, F1 Score = 0.4479
- logistic4: CCR = 0.9283, F1 Score = 0.4613
- logistic5: CCR = 0.929, F1 Score = 0.4444


### Writing the predicted probabilites to a file

```{r}
# prob_test <- data.frame(id = test_logistic$id, prob = round(pred_test, 3), responded = test_logistic$responded)
# prob_test <- arrange(prob_test, desc(prob))

prob_test <- data.frame(id = test_logistic$id, prob = pred_test)
write.csv(prob_test, "prob.csv", row.names=FALSE)
```



## Multiple Regression Model

Fitting a multiple regression model on `targdol` as response but only where `targdol>0`. 

### linear1

Fitting the linear model with targdol as reponse
```{r}
linear1 <- lm(targdol ~ . -slshist - ordhist - datead6 - datelp6 -ord4bfr, data = select(train_linear, -id))

summary(linear1)

pred_test <- predict(linear1, newdata = test_linear)

targdol_test <- data.frame(id = test_linear$id, targdol = pred_test)
```

### linear2

```{r}
linear2 <- lm(targdol ~ I(log(slstyr + K)) + I(log(slslyr + K)) +
                  I(log(sls2ago + K)) + I(log(sls3ago + K)) + 
                  I(log(slshist + K)) + I(log(ordtyr + K)) + 
                  I(log(ordlyr + K)) + I(log(ord2ago + K)) + 
                  I(log(ord3ago + K)) + I(log(ordhist + K)) +
                  I(log(sprord+K)) + I(log(falord+K)) + 
                  I(log(avg_amount + K)) + large_avg + active + 
                  lifetime + recency + lpurseason +
                  
                  recency:active + 
                  I(log(ord2ago + K)):I(log(ord3ago + K)) +
                I(log(slslyr + K)):I(log(sls2ago + K)) + 
                I(log(sls2ago + K)):I(log(sls3ago + K)) +
                I(log(ord3ago + K)):I(log(ord4bfr + K)) +
                  I(log(avg_amount + K)):recency + 
                  pur3yr 
              ,data = select(train_linear, -id))

summary(linear2)

pred_test <- predict(linear2, newdata = test_linear)

targdol_test <- data.frame(id = test_linear$id, targdol = pred_test)
```

### linear3

```{r}
linear3 <- lm(I(log(targdol+K)) ~ I(log(slstyr + K)) + I(log(slslyr + K)) +
                  I(log(sls2ago + K)) + I(log(sls3ago + K)) + 
                  I(log(slshist + K)) + I(log(ordtyr + K)) + 
                  I(log(ordlyr + K)) + I(log(ord2ago + K)) + 
                  I(log(ord3ago + K)) + I(log(ordhist + K)) +
                  I(log(sprord+K)) + I(log(falord+K)) + 
                  I(log(avg_amount + K)) + large_avg + active + 
                  lifetime + recency + lpurseason, 
              data = select(train_linear, -id))

summary(linear3)

pred_test <- predict(linear3, newdata = test_linear)
pred_test <- exp(pred_test)-K

targdol_test <- data.frame(id = test_linear$id, targdol = pred_test)
```

### linear4

```{r}
linear4 <- lm(I(log(targdol+K)) ~ I(log(slstyr + K)) + I(log(slslyr + K)) +
                  I(log(sls2ago + K)) + I(log(sls3ago + K)) + 
                  I(log(slshist + K)) + I(log(ordtyr + K)) + 
                  I(log(ordlyr + K)) + I(log(ord2ago + K)) + 
                  I(log(ord3ago + K)) + I(log(ordhist + K)) +
                  I(log(sprord+K)) + I(log(falord+K)) + 
                  I(log(avg_amount + K)) + large_avg + active + 
                  lifetime + recency + lpurseason + 
                  
                I(log(slstyr + K)):I(log(slslyr + K)) +
                I(log(slslyr + K)):I(log(sls2ago + K)) + 
                I(log(sls2ago + K)):I(log(sls3ago + K)) +
                I(log(sls3ago + K)):I(log(sls4bfr + K)) 
            
                # recency:active + lifetime:slscmp + 
                # I(log(slstyr + K)):I(log(ordtyr+ K)) + 
                # I(log(slstyr + K)):recency + 
                #     
                #     I(log(slslyr + K)):recency +
                #     I(log(sls2ago + K)):I(log(ord2ago + K)) + 
                #     I(log(avg_amount + K)):recency
              ,data = select(train_linear, -id))

summary(linear4)

pred_test <- predict(linear4, newdata = test_linear)
pred_test <- exp(pred_test)-K

targdol_test <- data.frame(id = test_linear$id, targdol = pred_test)
```


### linear5

```{r}
linear5 <- lm(targdol ~ I(log(slstyr + K)) + I(log(slslyr + K)) +
                  I(log(sls2ago + K)) + I(log(sls3ago + K)) + 
                  I(log(slshist + K)) + I(log(ordtyr + K)) + 
                  I(log(ordlyr + K)) + I(log(ord2ago + K)) + 
                  I(log(ord3ago + K)) + I(log(ordhist + K)) +
                  I(log(sprord+K)) + I(log(falord+K)) + 
                  I(log(avg_amount + K)) + large_avg + active + 
                  lifetime + recency + lpurseason + 

                I(log(slstyr + K)):I(log(slslyr + K)) +
                I(log(slslyr + K)):I(log(sls2ago + K)) +
                I(log(sls2ago + K)):I(log(sls3ago + K)) +
                I(log(sls3ago + K)):I(log(sls4bfr + K)) +
            
                recency:active + lifetime:slscmp +
                I(log(slstyr + K)):I(log(ordtyr+ K)) +
                I(log(slstyr + K)):recency +

                    I(log(slslyr + K)):recency +
                    I(log(sls2ago + K)):I(log(ord2ago + K)) +
                    I(log(avg_amount + K)):recency
              ,data = train_linear)

summary(linear5)

pred_test <- predict(linear5, newdata = test_linear)
# pred_test <- exp(pred_test)-K

targdol_test <- data.frame(id = test_linear$id, targdol = pred_test)
```


## Checking for interaction terms

Creating a matrix of all possible interaction terms
```{r}
all_interact_train <- model.matrix(~(slstyr + slslyr + 
                                         sls2ago + sls3ago + ordtyr + 
                                         ordlyr + ord2ago + ord3ago + 
                                         lpurseason + falord + sprord + recency +
                                         lifetime + active + avg_amount + 
                                         large_avg + pur3yr + slscmp + sls4bfr + 
                                         ord4bfr)^2, data = train_linear)

log_all_interact_train <- model.matrix(~(I(log(slstyr + K)) + I(log(slslyr + K))+
                I(log(sls2ago + K)) + I(log(sls3ago + K)) + I(log(sls4bfr + K)) +
                I(log(ordtyr + K)) + I(log(ordlyr + K)) + I(log(ord2ago + K)) + 
                I(log(ord3ago + K)) + I(log(sprord+K)) + I(log(falord+K)) + 
                I(log(avg_amount + K)) + large_avg + active + lifetime + 
                recency + lpurseason + avg_amount + pur3yr + slscmp)^2,
                data = train_linear)

all_interact_test <- model.matrix(~(slstyr + slslyr + 
                                         sls2ago + sls3ago + ordtyr + 
                                         ordlyr + ord2ago + ord3ago + 
                                         lpurseason + falord + sprord + recency +
                                         lifetime + active + avg_amount + 
                                         large_avg + pur3yr + slscmp + sls4bfr + 
                                         ord4bfr)^2, data = test_linear)

log_all_interact_test <- model.matrix(~(I(log(slstyr + K)) + I(log(slslyr + K))+
                I(log(sls2ago + K)) + I(log(sls3ago + K)) + I(log(sls4bfr + K)) +
                I(log(ordtyr + K)) + I(log(ordlyr + K)) + I(log(ord2ago + K)) + 
                I(log(ord3ago + K)) + I(log(sprord+K)) + I(log(falord+K)) + 
                I(log(avg_amount + K)) + large_avg + active + lifetime + 
                recency + lpurseason + avg_amount + pur3yr + slscmp)^2,
                data = test_linear)



# tr <- log(all_interact_pair+1)
tr <- log_all_interact_pair
# nsv <- nearZeroVar(tr, saveMetrics = TRUE)
# tr = tr[,!nsv$nzv]
tr_cor <- as.matrix(cor(tr))
tr_cor_melt <- arrange(melt(tr_cor), -abs(value))
tr_cor_melt <- filter(tr_cor_melt, Var1 != Var2)
tr_cor_melt <- tr_cor_melt[seq(from=1, to=nrow(tr_cor_melt), by=2),]
thresh = 0.7 # displaying only correlations above threshold
filter(tr_cor_melt, abs(value)>thresh)
```

Removing the variables which are highly correlated with another variable.
```{r}
# remove_cols <- filter(tr_cor_melt, abs(value)>thresh) %>% select(Var1) %>% unique()
# remove_cols$Var1 <- as.character(remove_cols$Var1)
# tr <- tr[, -which(colnames(tr) %in% remove_cols$Var1)]
```

#### Fitting a backwards stepwise regression with all the interaction variables.

Backwards stepwise regression on the log of targdol
```{r}
all_fit_log <- lm(log(train_linear$targdol+K) ~ . , data = data.frame(log_all_interact_train))
fit3 <- step(all_fit_log, direction = "both")
```

Performing Lasso regression on all possible pairs of interaction terms.
```{r}
lassocv <- cv.glmnet(log_all_interact_train, log(train_linear$targdol+K), alpha=1, nfold=3, lambda=seq(0,10,0.01))
lambdalasso=lassocv$lambda.min
print(lambdalasso)
plot(lassocv)
small.lambda.index <- which(lassocv$lambda == lassocv$lambda.min)
small.lambda.betas <- coef(lassocv$glmnet.fit)[,small.lambda.index]
print(small.lambda.betas)
plot(lassocv,xvar="lambda",label=TRUE, main="Coeffs of Lasso Regression", type="l", xlab=expression("log_lambda"), ylab="Coeff")
abline(h=0); 
abline(v=log(lassocv$lambda.min))
summary(lassocv)
```

Best subsets regression on targdol
```{r}
all_fit <- lm(train_linear$targdol ~ . , data = data.frame(all_interact_train))
fit4 <- step(all_fit, direction = "both")
```


VIFs for the stepwise regression model.
```{r}
vifs <- vif(bwd)
sort(vifs)
coefs <- names(vifs)
log_all_interact_pair_df <- data.frame(log_all_interact_pair)
log_all_interact_pair_df <- log_all_interact_pair_df[, which(names(log_all_interact_pair_df) %in% coefs)]

# best_bwd <- lm(train2$targdol ~ ., data = log_all_interact)
# summary(best_bwd)
# sort(vif(best_bwd), decreasing = TRUE)
# 
# remove <- names(vif(best_bwd)[vif(best_bwd)>1000])
# log_all_interact <- log_all_interact[, -which(names(log_all_interact) %in% remove)]
# 
# best_bwd <- lm(train2$targdol ~ ., data = log_all_interact)
# summary(best_bwd)
# sort(vif(best_bwd), decreasing = TRUE)
```

Best subsets regression on logarithm of targdol
```{r}
bestAIClog <- bestglm(data.frame(log_all_interact_pair_df, log(train$targdol+K)), IC="AIC")
summary(bestAIClog$BestModel)
```

This model has an adjusted $R^2$ of 0.1517.


Best subsets regression on targdol
```{r}
bestAIC <- bestglm(data.frame(log_all_interact_pair_df, train$targdol), IC="AIC")
summary(bestAIC$BestModel)
```








Fitting a linear model using the significant coefficients of the LASSO REGRESSION

```{r}
lmfit2 <- lm(I(log(targdol+1)) ~ I(log(slstyr + K)) + I(log(slslyr + K)) +
                I(log(sls2ago + K)) + I(log(sls3ago + K)) + I(log(slshist + K)) +
                I(log(ordtyr + K)) + I(log(ordlyr + K)) +
                I(log(ord2ago + K)) + I(log(ord3ago + K)) + I(log(ordhist + K)) +
                I(log(sprord+K)) + I(log(falord+K)) +
                I(log(avg_amount + K)) + large_avg + active + lifetime + recency + lpurseason + 
                I(log(slstyr + K)):I(log(slslyr + K)) + I(log(slstyr + K)):I(log(ordtyr + K)) + I(log(ordtyr + K)):I(log(ordlyr + K)) + I(log(ordtyr + K)):I(log(ord3ago + K)) + I(log(ordlyr + K)):I(log(ord3ago + K)) + I(log(sprord + K)):I(log(falord + K)) + large_avg:active + large_avg:lifetime,
                 data = select(train, -id))

summary(lmfit2)
```


Taking the non zero coefficients from lasso regression and performing linear regression on them.
```{r}
log_all_interact_df <- data.frame(log_all_interact)
log_all_interact_df <- log_all_interact_df[, which(gsub("\\.",":",names(log_all_interact_df)) %in% names(small.lambda.betas[small.lambda.betas>0]))]
all_interact_fit <- lm(I(log(train2$targdol+K)) ~ ., data = log_all_interact_df)

summary(all_interact_fit)
```

The $R^2$ is 0.052.

Backwards stepwise regression on all possible interaction terms
```{r}
all_interact_fit <- lm(train2$targdol ~ . , data = data.frame(log_all_interact))
bwd <- step(all_interact_fit, direction = "both")

bestAIClog_allinteract <- bestglm(data.frame(log_all_interact, log(train2$targdol)), IC="AIC")
summary(bestAIClog_allinteract$BestModel)
```

### Writing the predicted targdol to a file

```{r}
write.csv(targdol_test, "targdol.csv", row.names=FALSE)
```

## Evalutaing the fitted models

Here we will evaluate the different combinations of the logistic and linear regression models that we have tried so far and see which of these models gets the lowest mean squared prediction error (statistical criterion) and the highest payoff (financial criterion).

```{r}
targdol_test <- read.csv("targdol.csv")
prob_test <- read.csv("prob.csv")
```

### Statistical Criterion

```{r}
actual_targdol <- select(test_linear, id, targdol)
linear_model <- linear2
prob_targdol <- merge(prob_test, targdol_test, by = "id")
prob_targdol <- merge(prob_targdol, actual_targdol, by = "id")
names(prob_targdol) <- c("id", "prob", "pred_targdol", "actual_targdol")
prob_targdol <- mutate(prob_targdol, expected_targdol = prob*pred_targdol)
head(prob_targdol)
p <- length(linear_model$coefficients) - 1
n <- nrow(prob_targdol)
MSPE <- sum((prob_targdol$actual_targdol - 
                 prob_targdol$expected_targdol)^2)/(n-(p+1))
MSPE
```

### Financial Criterion

Sort in descending order of the expected targdol and grab the corresponding top 1000 actual targdol values.
```{r}
prob_targdol_arranged <- arrange(prob_targdol, desc(expected_targdol))
payoff <- sum(prob_targdol_arranged[1:1000, ]$actual_targdol)
payoff
```














